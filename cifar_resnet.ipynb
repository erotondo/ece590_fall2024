{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/akamaster/pytorch_resnet_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Framework (resnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ResNet-S Available: [20,32,44,*56*,110,1202]\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_dict = {\n",
    "    \"resnet56\": resnet56\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer/Evaluation (trainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.datasets as datasets\n",
    "#import resnet # Refers to resnet.py, aka above\n",
    "\n",
    "# Eliminate nondeterministic algorithm procedures\n",
    "cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Parse_args\" Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(pt_path=\"\",pretrain_flag=True,finetune_flag=False,eval_flag=False,\n",
    "               batch_size=128,print_freq=5,sam_flag=False,seg_path=\"\",mask_pad_param=0):\n",
    "    parser = argparse.ArgumentParser(description='Propert ResNets for CIFAR10 in pytorch')\n",
    "    #model_names = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "    model_names = ['resnet56']\n",
    "    \n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet56',\n",
    "                        choices=model_names,\n",
    "                        help='model architecture: ' + ' | '.join(model_names) +\n",
    "                        ' (default: resnet56)')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    # Modified to be number of epochs during ***FINETUNING***\n",
    "    # Previous, from scratch training, total epochs = 200\n",
    "    parser.add_argument('--epochs', default=25, type=int, metavar='N',\n",
    "                        help='number of epochs to run for funetuning')\n",
    "    # FOR TRAINING (from scratch)\n",
    "    # parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "    #                     help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=batch_size, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 128)')\n",
    "    # Scheduler adjusts currently, even during finetuning, \n",
    "    # assuming 'last_epoch' parameter is used\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    # Changed from default = 50, when training from scratch/200 epochs\n",
    "    parser.add_argument('--print-freq', '-p', default=print_freq, type=int,\n",
    "                        metavar='N', help='print frequency (default: 5)')\n",
    "    # MANUALLY SET TO LOCATION OF RESNET56 CHECKPOINT FOR PRETRAINED MODEL\n",
    "    parser.add_argument('--resume', default=pt_path, type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    # FLAG\n",
    "    parser.add_argument('--pt', '--pretrained', dest='pretrained', default=pretrain_flag, \n",
    "                        type=bool, metavar='PT_FLAG', help='use pre-trained model')\n",
    "    # FLAG\n",
    "    parser.add_argument('--ft', '--finetune', dest='finetune', default=finetune_flag,\n",
    "                        type=bool, metavar='FT_FLAG', \n",
    "                        help='finetune the model, location specified by [--resume PATH]')\n",
    "    # FLAG\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', default=eval_flag,\n",
    "                        type=bool, metavar='EVAL_FLAG', help='evaluate model on test set')\n",
    "    # parser.add_argument('--half', dest='half', action='store_true',\n",
    "    #                     help='use half-precision(16-bit) ')\n",
    "    parser.add_argument('--save-dir', dest='save_dir',\n",
    "                        help='The directory used to save the trained models',\n",
    "                        default=os.path.join('model_checkpoints','save_temp'), type=str)\n",
    "    ### ***IGNORE FOR NOW, COME BACK TO*** ###\n",
    "    # parser.add_argument('--save-every', dest='save_every',\n",
    "    #                     help='Saves checkpoints at every specified number of epochs',\n",
    "    #                     type=int, default=10)\n",
    "    # FLAG\n",
    "    parser.add_argument('--sam', '--sam_segmentation', dest='use_sam', default=sam_flag,\n",
    "                        type=bool, metavar='SAM_FLAG', help='use SAM for image segmentation')\n",
    "    parser.add_argument('--seg_model', dest='seg_checkpoint', default=seg_path, \n",
    "                        type=str, metavar='PATH', help='path to segmentation model (default: none)')\n",
    "    parser.add_argument('--mp', '--mask_padding_param', dest=\"mpp\", default=mask_pad_param, type=int, \n",
    "                        metavar='N', help='padding width to extend mask border during segmentation')\n",
    "\n",
    "    \n",
    "    # Trick .ipynb notebook into properly compiling parse_args with empty \"args\" parameter\n",
    "    config = parser.parse_args(args=[])\n",
    "    #config = parser.parse_args()  \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AverageMeter Object Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (precision@k) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % config['print_freq'] == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, test_loader, model, criterion, use_cuda, seg_tf=None, norm_tf=None):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "                \n",
    "            # Apply transformations during eval loop; segmentation, then normalization\n",
    "            if seg_tf:\n",
    "                input = torch.stack([seg_tf(input[i,:,:,:]) for i in range(input.shape[0])])\n",
    "            if norm_tf:\n",
    "                input = norm_tf(input)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input.float())\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % config['print_freq'] == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(test_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Model and Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from transformations import SAMSegmentationTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ViT_B SAM Model\n",
    "# sam_b = sam_model_registry[\"vit_b\"](checkpoint=\"eli_dev/seg_any_model/models/vit_b/sam_vit_b_01ec64.pth\")\n",
    "# mask_b_predictor = SamPredictor(sam_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAMSegmentationTransform(object):\n",
    "#     def __init__(self, mask_predictor, mask_padding=0):\n",
    "#         self.predictor = mask_predictor\n",
    "#         self.mask_padding = mask_padding\n",
    "#         # If desired to extend object masks with padding\n",
    "#         self.mask_pad_conv2d = None\n",
    "#         if mask_padding > 0:\n",
    "#             self.mask_pad_conv2d = nn.Conv2d(1, 1, kernel_size=(1+(2*mask_padding)), \n",
    "#                                              padding=\"same\", bias=False)\n",
    "#             self.mask_pad_conv2d.weight.data = torch.ones(1,1,(1+(2*mask_padding)),(1+(2*mask_padding)))\n",
    "        \n",
    "        \n",
    "#     def __call__(self, image):\n",
    "#         # Set image\n",
    "#         self.predictor.set_image(image)\n",
    "#         input_point = torch.Tensor([[16, 16]])\n",
    "#         input_label = torch.Tensor([1])\n",
    "#         masks, scores, logits = self.predictor.predict(\n",
    "#             point_coords=input_point,\n",
    "#             point_labels=input_label,\n",
    "#             multimask_output=True,\n",
    "#         )\n",
    "        \n",
    "#         # Identify best mask, extend borders if necessary, expand dims\n",
    "#         best_mask = masks[torch.argmax(scores),:,:]\n",
    "#         if self.mask_padding > 0:\n",
    "#             best_mask = self.mask_pad_conv2d(best_mask)\n",
    "#             best_mask[best_mask > 0] = 1\n",
    "#         best_mask = torch.stack((best_mask,)*3, axis=-1)\n",
    "        \n",
    "#         seg_img = image * best_mask\n",
    "#         seg_img[seg_img==0] = 255\n",
    "#         seg_img = seg_img.int()\n",
    "        \n",
    "#         return seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Main\" Function, aka where most of the sequential logic and controlled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pt_path=\"\",pretrain_flag=True,finetune_flag=False,eval_flag=False,\n",
    "         batch_size=128,print_freq=5,sam_flag=False,seg_path=\"\",mask_pad_param=0):\n",
    "    config = vars(parse_args(pt_path,pretrain_flag,finetune_flag,eval_flag,batch_size,\n",
    "                             print_freq,sam_flag,seg_path,mask_pad_param))\n",
    "    best_prec1 = 0 # Used during training/finetuning\n",
    "\n",
    "    # Check the save_dir exists or not\n",
    "    if not os.path.exists(config['save_dir']):\n",
    "        os.makedirs(config['save_dir'])\n",
    "        \n",
    "    # Check status of cuda, set device\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Load model architecture\n",
    "    model = torch.nn.DataParallel(resnet_dict[config['arch']]())\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint/pretrained model\n",
    "    # Always true for when pretrained model is desired (should be, anyways)\n",
    "    if config['resume']:\n",
    "        print(\"=> loading checkpoint '{}'\".format(config['resume']))\n",
    "        checkpoint = torch.load(config['resume'],map_location=device)\n",
    "        # config['start_epoch'] = checkpoint['epoch']   # <-    args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        if not config[\"pretrained\"]:\n",
    "            print(\"=> loaded checkpoint (epoch {})\"\n",
    "                    .format(checkpoint['epochs']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(config['resume']))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    seg_model = {}\n",
    "    image_segment_transform = None\n",
    "    if config['use_sam']:\n",
    "        sam_model_vers = config['seg_checkpoint'].split(\"/\")[3]\n",
    "        seg_model[\"sam\"] = sam_model_registry[sam_model_vers](checkpoint=config['seg_checkpoint'])\n",
    "        seg_model[\"mask_predictor\"] = SamPredictor(seg_model[\"sam\"])\n",
    "        \n",
    "        image_segment_transform = SAMSegmentationTransform(seg_model[\"mask_predictor\"],config['mpp'])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./datasets', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]), # Equivalent to .ToTensor(), now deprecated\n",
    "            normalize,\n",
    "        ]), download=True),\n",
    "        batch_size=config['batch_size'], shuffle=True,\n",
    "        num_workers=config['workers'], pin_memory=True)\n",
    "\n",
    "    # test_loader = torch.utils.data.DataLoader(\n",
    "    #     datasets.CIFAR10(root='./datasets', train=False, transform=transforms.Compose([\n",
    "    #         transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]), # Equivalent to .ToTensor(), now deprecated\n",
    "    #         image_segment_transform,normalize, # Need to segment before normalizing!\n",
    "    #     ])),\n",
    "    #     batch_size=config['batch_size'], shuffle=False,\n",
    "    #     num_workers=config['workers'], pin_memory=True)\n",
    "    \n",
    "    # During evaluation, segmentation and normalization transforms have been moved to within the evaluation function\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./datasets', train=False, transform=transforms.Compose([\n",
    "            transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]), # Equivalent to .ToTensor(), now deprecated\n",
    "            #image_segment_transform,normalize, # Need to segment before normalizing!\n",
    "        ])),\n",
    "        batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['workers'], pin_memory=True)\n",
    "    \n",
    "    # for batch_idx, (test_data, test_targets) in enumerate(test_loader):\n",
    "    #     for i in range(0, test_loader.batch_size-1):\n",
    "    #         #print(test_data.shape)\n",
    "    #         #print(test_data[i].shape)\n",
    "    #         segmented = torch.stack([image_segment_transform(test_data[i,:,:,:]) for i in range(test_data.shape[0])])\n",
    "    #         segmented = normalize(segmented)\n",
    "    #         print(segmented.shape)\n",
    "    #         #seg_img = np.array(segmented[i])\n",
    "    #         #print(seg_img.shape)\n",
    "    #         for j in range(segmented.shape[0]):\n",
    "    #             cur_img = np.array(test_data[j])\n",
    "    #             cur_img = cur_img.transpose((1,2,0))\n",
    "    #             plt.imshow(cur_img)\n",
    "    #             plt.show()\n",
    "    #             cur_seg = np.array(segmented[j])\n",
    "    #             cur_seg = cur_seg.transpose((1,2,0))\n",
    "    #             plt.imshow(cur_seg)\n",
    "    #             plt.show()\n",
    "    #         break\n",
    "    # # data, target = test_loader[0]\n",
    "    # # print(data.shape)\n",
    "    #     break\n",
    "    # return\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "    optimizer = torch.optim.SGD(model.parameters(),config['lr'],\n",
    "                                    momentum=config['momentum'],\n",
    "                                    weight_decay=config['weight_decay'])\n",
    "\n",
    "    # Will need to adjust to allow for resumed training if not only using pretrained models\n",
    "    start_epoch = 200 if config['pretrained'] else 0\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[100, 150])\n",
    "    lr_scheduler.last_epoch = start_epoch - 1\n",
    "\n",
    "    # if args.arch in ['resnet1202', 'resnet110']:\n",
    "    #     # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
    "    #     # then switch back. In this setup it will correspond for first epoch.\n",
    "    #     for param_group in optimizer.param_groups:\n",
    "    #         param_group['lr'] = args.lr*0.1\n",
    "\n",
    "    if config['evaluate']:\n",
    "        evaluate(config, test_loader, model, criterion, use_cuda, \n",
    "                seg_tf=image_segment_transform, norm_tf=normalize)\n",
    "    else:\n",
    "        # Will need to adjust to allow for resumed training if not only using pretrained models\n",
    "        for epoch in range(start_epoch, start_epoch + config['epochs']):\n",
    "            # train for one epoch\n",
    "            print('Current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "            train(config, train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            prec1 = evaluate(config, test_loader, model, criterion, use_cuda,\n",
    "                             seg_tf=image_segment_transform, norm_tf=normalize)\n",
    "\n",
    "            # remember best prec@1 and save checkpoint\n",
    "            is_best = prec1 > best_prec1\n",
    "            best_prec1 = max(prec1, best_prec1)\n",
    "            \n",
    "            # if epoch > 0 and epoch % args.save_every == 0:\n",
    "            #         save_checkpoint({\n",
    "            #             'epoch': epoch + 1,\n",
    "            #             'state_dict': model.state_dict(),\n",
    "            #             'best_prec1': best_prec1,\n",
    "            #         }, is_best, filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
    "\n",
    "            save_checkpoint({\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "            }, is_best, filename=os.path.join(config['save_dir'], 'model.th'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'model_checkpoints/pretrained/resnet56-4bfd9763.th'\n",
      "Files already downloaded and verified\n",
      "Test: [0/1250]\tTime 151.413 (151.413)\tLoss 0.0160 (0.0160)\tPrec@1 100.000 (100.000)\n",
      "Test: [1/1250]\tTime 133.577 (142.495)\tLoss 0.0444 (0.0302)\tPrec@1 100.000 (100.000)\n",
      "Test: [2/1250]\tTime 165.269 (150.086)\tLoss 0.1923 (0.0842)\tPrec@1 87.500 (95.833)\n",
      "Test: [3/1250]\tTime 151.443 (150.425)\tLoss 1.0068 (0.3149)\tPrec@1 87.500 (93.750)\n",
      "Test: [4/1250]\tTime 129.638 (146.268)\tLoss 2.7621 (0.8043)\tPrec@1 50.000 (85.000)\n",
      "Test: [5/1250]\tTime 128.229 (143.261)\tLoss 1.1315 (0.8588)\tPrec@1 75.000 (83.333)\n",
      "Test: [6/1250]\tTime 131.845 (141.631)\tLoss 0.3639 (0.7881)\tPrec@1 75.000 (82.143)\n",
      "Test: [7/1250]\tTime 134.630 (140.756)\tLoss 1.5383 (0.8819)\tPrec@1 62.500 (79.688)\n",
      "Test: [8/1250]\tTime 130.065 (139.568)\tLoss 0.3312 (0.8207)\tPrec@1 87.500 (80.556)\n",
      "Test: [9/1250]\tTime 128.117 (138.423)\tLoss 0.7280 (0.8114)\tPrec@1 50.000 (77.500)\n",
      "Test: [10/1250]\tTime 147.115 (139.213)\tLoss 1.2401 (0.8504)\tPrec@1 62.500 (76.136)\n",
      "Test: [11/1250]\tTime 131.891 (138.603)\tLoss 1.4635 (0.9015)\tPrec@1 50.000 (73.958)\n",
      "Test: [12/1250]\tTime 126.748 (137.691)\tLoss 1.1006 (0.9168)\tPrec@1 75.000 (74.038)\n",
      "Test: [13/1250]\tTime 128.519 (137.036)\tLoss 0.1472 (0.8619)\tPrec@1 87.500 (75.000)\n",
      "Test: [14/1250]\tTime 136.308 (136.987)\tLoss 0.8431 (0.8606)\tPrec@1 87.500 (75.833)\n",
      "Test: [15/1250]\tTime 133.096 (136.744)\tLoss 2.9123 (0.9888)\tPrec@1 50.000 (74.219)\n",
      "Test: [16/1250]\tTime 143.629 (137.149)\tLoss 1.9306 (1.0442)\tPrec@1 62.500 (73.529)\n",
      "Test: [17/1250]\tTime 153.538 (138.059)\tLoss 1.1398 (1.0495)\tPrec@1 87.500 (74.306)\n",
      "Test: [18/1250]\tTime 133.469 (137.818)\tLoss 1.1271 (1.0536)\tPrec@1 62.500 (73.684)\n",
      "Test: [19/1250]\tTime 145.978 (138.226)\tLoss 0.9660 (1.0492)\tPrec@1 75.000 (73.750)\n",
      "Test: [20/1250]\tTime 128.777 (137.776)\tLoss 0.2710 (1.0122)\tPrec@1 87.500 (74.405)\n",
      "Test: [21/1250]\tTime 125.402 (137.213)\tLoss 2.2105 (1.0666)\tPrec@1 62.500 (73.864)\n",
      "Test: [22/1250]\tTime 129.147 (136.863)\tLoss 1.6336 (1.0913)\tPrec@1 50.000 (72.826)\n",
      "Test: [23/1250]\tTime 129.952 (136.575)\tLoss 0.8875 (1.0828)\tPrec@1 87.500 (73.438)\n",
      "Test: [24/1250]\tTime 126.748 (136.182)\tLoss 2.1058 (1.1237)\tPrec@1 75.000 (73.500)\n",
      "Test: [25/1250]\tTime 129.615 (135.929)\tLoss 1.1054 (1.1230)\tPrec@1 75.000 (73.558)\n",
      "Test: [26/1250]\tTime 136.840 (135.963)\tLoss 0.5673 (1.1024)\tPrec@1 62.500 (73.148)\n",
      "Test: [27/1250]\tTime 139.193 (136.078)\tLoss 0.7705 (1.0906)\tPrec@1 87.500 (73.661)\n",
      "Test: [28/1250]\tTime 130.788 (135.896)\tLoss 2.5181 (1.1398)\tPrec@1 62.500 (73.276)\n",
      "Test: [29/1250]\tTime 142.412 (136.113)\tLoss 0.8240 (1.1293)\tPrec@1 87.500 (73.750)\n",
      "Test: [30/1250]\tTime 129.911 (135.913)\tLoss 1.1311 (1.1293)\tPrec@1 87.500 (74.194)\n",
      "Test: [31/1250]\tTime 137.153 (135.952)\tLoss 2.3700 (1.1681)\tPrec@1 37.500 (73.047)\n",
      "Test: [32/1250]\tTime 143.306 (136.175)\tLoss 0.4106 (1.1452)\tPrec@1 87.500 (73.485)\n",
      "Test: [33/1250]\tTime 144.500 (136.419)\tLoss 0.9895 (1.1406)\tPrec@1 75.000 (73.529)\n",
      "Test: [34/1250]\tTime 144.864 (136.661)\tLoss 1.2926 (1.1449)\tPrec@1 75.000 (73.571)\n",
      "Test: [35/1250]\tTime 143.975 (136.864)\tLoss 1.0409 (1.1420)\tPrec@1 62.500 (73.264)\n",
      "Test: [36/1250]\tTime 151.254 (137.253)\tLoss 0.4343 (1.1229)\tPrec@1 87.500 (73.649)\n",
      "Test: [37/1250]\tTime 137.654 (137.263)\tLoss 0.5664 (1.1083)\tPrec@1 87.500 (74.013)\n",
      "Test: [38/1250]\tTime 139.374 (137.318)\tLoss 2.0654 (1.1328)\tPrec@1 50.000 (73.397)\n",
      "Test: [39/1250]\tTime 125.690 (137.027)\tLoss 2.2504 (1.1607)\tPrec@1 37.500 (72.500)\n",
      "Test: [40/1250]\tTime 126.277 (136.765)\tLoss 0.8432 (1.1530)\tPrec@1 87.500 (72.866)\n",
      "Test: [41/1250]\tTime 139.802 (136.837)\tLoss 1.0239 (1.1499)\tPrec@1 75.000 (72.917)\n",
      "Test: [42/1250]\tTime 153.440 (137.223)\tLoss 1.7098 (1.1629)\tPrec@1 62.500 (72.674)\n",
      "Test: [43/1250]\tTime 134.715 (137.166)\tLoss 1.3680 (1.1676)\tPrec@1 75.000 (72.727)\n",
      "Test: [44/1250]\tTime 127.565 (136.953)\tLoss 0.4195 (1.1510)\tPrec@1 87.500 (73.056)\n",
      "Test: [45/1250]\tTime 139.655 (137.011)\tLoss 1.0998 (1.1499)\tPrec@1 75.000 (73.098)\n",
      "Test: [46/1250]\tTime 126.903 (136.796)\tLoss 1.0019 (1.1467)\tPrec@1 87.500 (73.404)\n",
      "Test: [47/1250]\tTime 126.929 (136.591)\tLoss 2.2098 (1.1689)\tPrec@1 62.500 (73.177)\n",
      "Test: [48/1250]\tTime 138.159 (136.623)\tLoss 0.0841 (1.1467)\tPrec@1 100.000 (73.724)\n",
      "Test: [49/1250]\tTime 148.237 (136.855)\tLoss 0.0631 (1.1251)\tPrec@1 100.000 (74.250)\n",
      "Test: [50/1250]\tTime 142.320 (136.962)\tLoss 1.7601 (1.1375)\tPrec@1 62.500 (74.020)\n",
      "Test: [51/1250]\tTime 146.182 (137.140)\tLoss 1.3929 (1.1424)\tPrec@1 62.500 (73.798)\n",
      "Test: [52/1250]\tTime 148.899 (137.361)\tLoss 4.7760 (1.2110)\tPrec@1 50.000 (73.349)\n",
      "Test: [53/1250]\tTime 147.895 (137.556)\tLoss 2.1164 (1.2277)\tPrec@1 50.000 (72.917)\n",
      "Test: [54/1250]\tTime 158.212 (137.932)\tLoss 1.6154 (1.2348)\tPrec@1 62.500 (72.727)\n",
      "Test: [55/1250]\tTime 159.057 (138.309)\tLoss 0.2141 (1.2166)\tPrec@1 87.500 (72.991)\n",
      "Test: [56/1250]\tTime 158.594 (138.665)\tLoss 1.3276 (1.2185)\tPrec@1 37.500 (72.368)\n",
      "Test: [57/1250]\tTime 156.480 (138.972)\tLoss 1.9640 (1.2314)\tPrec@1 75.000 (72.414)\n",
      "Test: [58/1250]\tTime 149.154 (139.145)\tLoss 1.7417 (1.2400)\tPrec@1 62.500 (72.246)\n",
      "Test: [59/1250]\tTime 128.520 (138.968)\tLoss 1.5400 (1.2450)\tPrec@1 50.000 (71.875)\n",
      "Test: [60/1250]\tTime 125.138 (138.741)\tLoss 3.2093 (1.2772)\tPrec@1 50.000 (71.516)\n",
      "Test: [61/1250]\tTime 124.002 (138.503)\tLoss 0.1826 (1.2596)\tPrec@1 87.500 (71.774)\n",
      "Test: [62/1250]\tTime 126.531 (138.313)\tLoss 1.5110 (1.2636)\tPrec@1 75.000 (71.825)\n",
      "Test: [63/1250]\tTime 124.424 (138.096)\tLoss 2.5614 (1.2838)\tPrec@1 62.500 (71.680)\n",
      "Test: [64/1250]\tTime 129.118 (137.958)\tLoss 0.7340 (1.2754)\tPrec@1 87.500 (71.923)\n",
      "Test: [65/1250]\tTime 132.184 (137.871)\tLoss 1.2112 (1.2744)\tPrec@1 75.000 (71.970)\n",
      "Test: [66/1250]\tTime 135.467 (137.835)\tLoss 0.4371 (1.2619)\tPrec@1 87.500 (72.201)\n",
      "Test: [67/1250]\tTime 147.608 (137.979)\tLoss 0.1735 (1.2459)\tPrec@1 87.500 (72.426)\n",
      "Test: [68/1250]\tTime 138.776 (137.990)\tLoss 3.3003 (1.2757)\tPrec@1 50.000 (72.101)\n",
      "Test: [69/1250]\tTime 156.176 (138.250)\tLoss 1.2141 (1.2748)\tPrec@1 87.500 (72.321)\n",
      "Test: [70/1250]\tTime 168.689 (138.679)\tLoss 2.6636 (1.2944)\tPrec@1 50.000 (72.007)\n",
      "Test: [71/1250]\tTime 159.100 (138.962)\tLoss 1.8686 (1.3023)\tPrec@1 62.500 (71.875)\n",
      "Test: [72/1250]\tTime 163.248 (139.295)\tLoss 0.2311 (1.2877)\tPrec@1 87.500 (72.089)\n",
      "Test: [73/1250]\tTime 162.298 (139.606)\tLoss 0.0090 (1.2704)\tPrec@1 100.000 (72.466)\n",
      "Test: [74/1250]\tTime 146.646 (139.700)\tLoss 0.6986 (1.2628)\tPrec@1 87.500 (72.667)\n",
      "Test: [75/1250]\tTime 134.833 (139.636)\tLoss 0.8327 (1.2571)\tPrec@1 62.500 (72.533)\n",
      "Test: [76/1250]\tTime 130.668 (139.519)\tLoss 0.0497 (1.2414)\tPrec@1 100.000 (72.890)\n",
      "Test: [77/1250]\tTime 133.883 (139.447)\tLoss 0.9582 (1.2378)\tPrec@1 75.000 (72.917)\n",
      "Test: [78/1250]\tTime 136.031 (139.404)\tLoss 1.1126 (1.2362)\tPrec@1 87.500 (73.101)\n",
      "Test: [79/1250]\tTime 134.281 (139.340)\tLoss 0.2607 (1.2240)\tPrec@1 87.500 (73.281)\n",
      "Test: [80/1250]\tTime 138.125 (139.325)\tLoss 0.0447 (1.2094)\tPrec@1 100.000 (73.611)\n",
      "Test: [81/1250]\tTime 136.571 (139.291)\tLoss 1.6879 (1.2153)\tPrec@1 62.500 (73.476)\n",
      "Test: [82/1250]\tTime 147.716 (139.393)\tLoss 1.5125 (1.2189)\tPrec@1 87.500 (73.645)\n",
      "Test: [83/1250]\tTime 135.138 (139.342)\tLoss 1.0341 (1.2167)\tPrec@1 87.500 (73.810)\n",
      "Test: [84/1250]\tTime 135.604 (139.298)\tLoss 1.8682 (1.2243)\tPrec@1 75.000 (73.824)\n",
      "Test: [85/1250]\tTime 135.797 (139.257)\tLoss 4.8651 (1.2667)\tPrec@1 37.500 (73.401)\n",
      "Test: [86/1250]\tTime 135.174 (139.210)\tLoss 0.6856 (1.2600)\tPrec@1 62.500 (73.276)\n",
      "Test: [87/1250]\tTime 144.379 (139.269)\tLoss 0.0607 (1.2464)\tPrec@1 100.000 (73.580)\n",
      "Test: [88/1250]\tTime 852.903 (147.287)\tLoss 3.1523 (1.2678)\tPrec@1 62.500 (73.455)\n",
      "Test: [89/1250]\tTime 1791.197 (165.553)\tLoss 0.7084 (1.2616)\tPrec@1 87.500 (73.611)\n",
      "Test: [90/1250]\tTime 1343.571 (178.498)\tLoss 2.2555 (1.2725)\tPrec@1 75.000 (73.626)\n",
      "Test: [91/1250]\tTime 3294.832 (212.371)\tLoss 0.0442 (1.2591)\tPrec@1 100.000 (73.913)\n",
      "Test: [92/1250]\tTime 8332.917 (299.689)\tLoss 0.9577 (1.2559)\tPrec@1 75.000 (73.925)\n",
      "Test: [93/1250]\tTime 137.758 (297.966)\tLoss 1.4055 (1.2575)\tPrec@1 62.500 (73.803)\n",
      "Test: [94/1250]\tTime 4587.266 (343.117)\tLoss 0.0760 (1.2450)\tPrec@1 100.000 (74.079)\n",
      "Test: [95/1250]\tTime 4313.153 (384.472)\tLoss 1.2918 (1.2455)\tPrec@1 62.500 (73.958)\n",
      "Test: [96/1250]\tTime 4020.163 (421.953)\tLoss 1.7388 (1.2506)\tPrec@1 62.500 (73.840)\n",
      "Test: [97/1250]\tTime 3226.089 (450.567)\tLoss 0.2061 (1.2400)\tPrec@1 87.500 (73.980)\n",
      "Test: [98/1250]\tTime 2817.469 (474.475)\tLoss 0.6470 (1.2340)\tPrec@1 75.000 (73.990)\n",
      "Test: [99/1250]\tTime 2936.205 (499.092)\tLoss 3.7616 (1.2592)\tPrec@1 37.500 (73.625)\n",
      "Test: [100/1250]\tTime 3536.202 (529.162)\tLoss 0.8063 (1.2548)\tPrec@1 75.000 (73.639)\n",
      "Test: [101/1250]\tTime 1300.475 (536.724)\tLoss 2.5487 (1.2674)\tPrec@1 75.000 (73.652)\n",
      "Test: [102/1250]\tTime 134.885 (532.823)\tLoss 1.0416 (1.2652)\tPrec@1 62.500 (73.544)\n",
      "Test: [103/1250]\tTime 130.828 (528.958)\tLoss 0.3302 (1.2563)\tPrec@1 87.500 (73.678)\n"
     ]
    }
   ],
   "source": [
    "# eli_dev/seg_any_model/models/vit_h/sam_vit_h_4b8939.pth # 8 images: about 3 min, 36 sec\n",
    "# eli_dev/seg_any_model/models/vit_l/sam_vit_l_0b3195.pth # 8 images: about 2 min, 41 sec\n",
    "# eli_dev/seg_any_model/models/vit_b/sam_vit_b_01ec64.pth # 8 images: about 1 min, 16 sec\n",
    "main(pt_path=\"model_checkpoints/pretrained/resnet56-4bfd9763.th\",pretrain_flag=True,\n",
    "     finetune_flag=False,eval_flag=True,sam_flag=True,batch_size=8,print_freq=1,\n",
    "     seg_path=\"eli_dev/seg_any_model/models/vit_l/sam_vit_l_0b3195.pth\",mask_pad_param=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
