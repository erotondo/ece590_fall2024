{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/akamaster/pytorch_resnet_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Framework (resnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ResNet-S Available: [20,32,44,*56*,110,1202]\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_dict = {\n",
    "    \"resnet56\": resnet56\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer/Evaluation (trainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "#from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.datasets as datasets\n",
    "#import resnet # Refers to resnet.py, aka above\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as pr_fscore_mtrc\n",
    "\n",
    "# Eliminate nondeterministic algorithm procedures\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Parse_args\" Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(pt_path=\"\",pretrain_flag=True,finetune_flag=False,eval_flag=False,\n",
    "               batch_size=128,print_freq=5,sam_flag=False,seg_path=\"\",mask_pad_param=0):\n",
    "    parser = argparse.ArgumentParser(description='Proper ResNets for CIFAR10 in pytorch')\n",
    "    #model_names = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "    model_names = ['resnet56']\n",
    "    \n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet56',\n",
    "                        choices=model_names,\n",
    "                        help='model architecture: ' + ' | '.join(model_names) +\n",
    "                        ' (default: resnet56)')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    # Modified to be number of epochs during ***FINETUNING***\n",
    "    # Previous, from scratch training, total epochs = 200\n",
    "    parser.add_argument('--epochs', default=25, type=int, metavar='N',\n",
    "                        help='number of epochs to run for funetuning')\n",
    "    # FOR TRAINING (from scratch)\n",
    "    # parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "    #                     help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=batch_size, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 128)')\n",
    "    # Scheduler adjusts currently, even during finetuning, \n",
    "    # assuming 'last_epoch' parameter is used\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    # Changed from default = 50, when training from scratch/200 epochs\n",
    "    parser.add_argument('--print-freq', '-p', default=print_freq, type=int,\n",
    "                        metavar='N', help='print frequency (default: 5)')\n",
    "    # MANUALLY SET TO LOCATION OF RESNET56 CHECKPOINT FOR PRETRAINED MODEL\n",
    "    parser.add_argument('--resume', default=pt_path, type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    # FLAG\n",
    "    parser.add_argument('--pt', '--pretrained', dest='pretrained', default=pretrain_flag, \n",
    "                        type=bool, metavar='PT_FLAG', help='use pre-trained model')\n",
    "    # FLAG\n",
    "    parser.add_argument('--ft', '--finetune', dest='finetune', default=finetune_flag,\n",
    "                        type=bool, metavar='FT_FLAG', \n",
    "                        help='finetune the model, location specified by [--resume PATH]')\n",
    "    # FLAG\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', default=eval_flag,\n",
    "                        type=bool, metavar='EVAL_FLAG', help='evaluate model on test set')\n",
    "    # parser.add_argument('--half', dest='half', action='store_true',\n",
    "    #                     help='use half-precision(16-bit) ')\n",
    "    parser.add_argument('--save-dir', dest='save_dir',\n",
    "                        help='The directory used to save the trained models',\n",
    "                        default=os.path.join('model_checkpoints','save_temp'), type=str)\n",
    "    ### ***IGNORE FOR NOW, COME BACK TO*** ###\n",
    "    # parser.add_argument('--save-every', dest='save_every',\n",
    "    #                     help='Saves checkpoints at every specified number of epochs',\n",
    "    #                     type=int, default=10)\n",
    "    # FLAG\n",
    "    parser.add_argument('--sam', '--sam_segmentation', dest='use_sam', default=sam_flag,\n",
    "                        type=bool, metavar='SAM_FLAG', help='use SAM for image segmentation')\n",
    "    parser.add_argument('--seg_model', dest='seg_checkpoint', default=seg_path, \n",
    "                        type=str, metavar='PATH', help='path to segmentation model (default: none)')\n",
    "    parser.add_argument('--mp', '--mask_padding_param', dest=\"mpp\", default=mask_pad_param, type=int, \n",
    "                        metavar='N', help='padding width to extend mask border during segmentation')\n",
    "\n",
    "    \n",
    "    # Trick .ipynb notebook into properly compiling parse_args with empty \"args\" parameter\n",
    "    config = parser.parse_args(args=[])\n",
    "    #config = parser.parse_args()  \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Number to Name Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num_to_name_dict = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AverageMeter Object Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (precision@k) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_f1_scores(y_target, t_pred, end_flag=False):\n",
    "    prec, rcll, f1sc, _ = pr_fscore_mtrc(y_target,t_pred,labels=np.arange(10,dtype=int),\n",
    "                                         average=None,zero_division=0)\n",
    "    print()\n",
    "    if end_flag:\n",
    "        print(\"Final Class F1-Scores:\")\n",
    "    else:\n",
    "        print(\"Intermediate Class F1-Scores:\")\n",
    "    for c in np.arange(10,dtype=int):\n",
    "        print(class_num_to_name_dict[c] + \": \" + str(round(f1sc[c],4)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % config['print_freq'] == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, test_loader, model, criterion, use_cuda, seg_tf=None, norm_tf=None):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    running_targets = []\n",
    "    running_preds = []\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            # Add current batch targets to running list of targets for computing class F1-Scores\n",
    "            running_targets.extend(target.tolist())\n",
    "            \n",
    "            # Apply transformations during eval loop; segmentation, then normalization\n",
    "            if seg_tf:\n",
    "                input = torch.stack([seg_tf(input[i,:,:,:]) for i in range(input.shape[0])])\n",
    "            if norm_tf:\n",
    "                input = norm_tf(input)\n",
    "                \n",
    "            if use_cuda:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input.float())\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Add current batch predictions to running list of predictions for computing class F1-Scores\n",
    "            _, pred = output.topk(1, 1, True, True)\n",
    "            running_preds.extend(pred.squeeze().tolist())\n",
    "            \n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % config['print_freq'] == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(test_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "                \n",
    "            if i % (config['print_freq']*10) == 0:\n",
    "                print_class_f1_scores(running_targets,running_preds)\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "    print_class_f1_scores(running_targets,running_preds,end_flag=True)\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Model and Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rembg import remove\n",
    "from PIL import Image, ImageDraw, ImageFilter, ImageChops\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ViT_B SAM Model\n",
    "# sam_b = sam_model_registry[\"vit_b\"](checkpoint=\"eli_dev/seg_any_model/models/vit_b/sam_vit_b_01ec64.pth\")\n",
    "# mask_b_predictor = SamPredictor(sam_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAMSegmentationTransform(object):\n",
    "#     def __init__(self, mask_predictor, mask_padding=0):\n",
    "#         self.predictor = mask_predictor\n",
    "#         self.mask_padding = mask_padding\n",
    "#         # If desired to extend object masks with padding\n",
    "#         self.mask_pad_conv2d = None\n",
    "#         if mask_padding > 0:\n",
    "#             self.mask_pad_conv2d = nn.Conv2d(1, 1, kernel_size=(1+(2*mask_padding)), \n",
    "#                                              padding=\"same\", bias=False)\n",
    "#             self.mask_pad_conv2d.weight.data = torch.ones(1,1,(1+(2*mask_padding)),(1+(2*mask_padding)))\n",
    "        \n",
    "        \n",
    "#     def __call__(self, image):\n",
    "#         # Set image\n",
    "#         self.predictor.set_image(image)\n",
    "#         input_point = torch.Tensor([[16, 16]])\n",
    "#         input_label = torch.Tensor([1])\n",
    "#         masks, scores, logits = self.predictor.predict(\n",
    "#             point_coords=input_point,\n",
    "#             point_labels=input_label,\n",
    "#             multimask_output=True,\n",
    "#         )\n",
    "        \n",
    "#         # Identify best mask, extend borders if necessary, expand dims\n",
    "#         best_mask = masks[torch.argmax(scores),:,:]\n",
    "#         if self.mask_padding > 0:\n",
    "#             best_mask = self.mask_pad_conv2d(best_mask)\n",
    "#             best_mask[best_mask > 0] = 1\n",
    "#         best_mask = torch.stack((best_mask,)*3, axis=-1)\n",
    "        \n",
    "#         seg_img = image * best_mask\n",
    "#         seg_img[seg_img==0] = 255\n",
    "#         seg_img = seg_img.int()\n",
    "        \n",
    "#         return seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Main\" Function, aka where most of the sequential logic and controlled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(inputFolder,outputFolder):\n",
    "    #adding center focus that won't be removed\n",
    "    def createEdgeMask(image, borderThickness=50):\n",
    "        width, height = image.size\n",
    "        mask = Image.new(\"L\", (width, height), 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.rectangle(\n",
    "            [borderThickness, borderThickness, width - borderThickness, height - borderThickness],\n",
    "            fill=255\n",
    "        )\n",
    "        mask = ImageChops.invert(mask)\n",
    "        return mask\n",
    "    #\n",
    "    def imageMask(inputPath, outputPath, borderThickness=50):\n",
    "        with open(inputPath, 'rb') as inputFile:\n",
    "            inputData = inputFile.read()\n",
    "            outputData = remove(inputData)\n",
    "        originalImage = Image.open(inputPath).convert(\"RGBA\")\n",
    "        processedImage = Image.open(io.BytesIO(outputData)).convert(\"RGBA\")\n",
    "        edgeMask = createEdgeMask(originalImage, borderThickness=borderThickness)\n",
    "        result = Image.composite(processedImage, originalImage, edgeMask)\n",
    "        processedImage.save(outputPath)\n",
    "\n",
    "    print(\"Masking...\")\n",
    "    for fileName in os.listdir(inputFolder):\n",
    "        if fileName.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "            inputPath = os.path.join(inputFolder, fileName)\n",
    "            outputPath = os.path.join(outputFolder, fileName)\n",
    "            print(\"Processing: \" +str(fileName))\n",
    "            imageMask(inputPath, outputPath, borderThickness=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patched_3.png\n",
      "Processing patched_2.png\n",
      "Processing patched_0.png\n",
      "Processing patched_1.png\n",
      "Processing patched_5.png\n",
      "Processing patched_4.png\n",
      "Processing patched_6.png\n",
      "Processing patched_7.png\n",
      "Processing patched_11.png\n",
      "Processing patched_10.png\n",
      "Processing patched_12.png\n",
      "Processing patched_13.png\n",
      "Processing patched_14.png\n",
      "Processing patched_15.png\n",
      "Processing patched_9.png\n",
      "Processing patched_8.png\n"
     ]
    }
   ],
   "source": [
    "main(inputFolder=\"dataset\",outputFolder=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Environment Unavailable; Running on CPU\n",
      "=> loading checkpoint 'model_checkpoints/pretrained/resnet56-4bfd9763.th'\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# eli_dev/seg_any_model/models/vit_h/sam_vit_h_4b8939.pth # 8 images: about 3 min, 36 sec\n",
    "# eli_dev/seg_any_model/models/vit_l/sam_vit_l_0b3195.pth # 8 images: about 2 min, 41 sec\n",
    "# eli_dev/seg_any_model/models/vit_b/sam_vit_b_01ec64.pth # 8 images: about 1 min, 16 sec\n",
    "main(pt_path=\"model_checkpoints/pretrained/resnet56-4bfd9763.th\",pretrain_flag=True,\n",
    "     finetune_flag=False,eval_flag=True,sam_flag=True,batch_size=8,print_freq=1,\n",
    "     seg_path=\"eli_dev/seg_any_model/models/vit_l/sam_vit_l_0b3195.pth\",mask_pad_param=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
